# Background & History

## Origins: an observer‑first toy model

This project started as a toy model built on observer‑first principles. I endeavored to take tagged corpus materials and turn them into mathematical objects that I could simulate to understand various layers of narrative point of view and how writers construct scenes, characters, and entire stories using various narrative techniques. I was interested in, for example, the number of temporal narrator “selves” nested in a single, extremely long, winding sentence by Marcel Proust. This quickly turned into just saying “heck it” and modeling human cognition by creating cognitive typologies that could be mathematically simulated in dyads and triads (**NOT SCIENTIFICALLY TESTED OR VALIDATED**), which went all the way Bananasville, and I was building computationally simulatable universes (**ALSO NOT SCIENTIFICALLY TESTED OR VALIDATED**).

The thing about a formalized toy model of this nature is that it provides a speculative mathematical playground for theory. However, any form of qualitative or quantitative research and testing to empirically validate predictions or hypotheses associated with it would be an immense research program just to parameterize and test even small slivers of the model.

I'm a card‑carrying fiction writer. You have to understand: my furthest‑reaching academic achievement is an advanced degree in which the word “fiction” is on my diploma. There is a by‑definition protective layer built into this specialty; fiction doesn’t have to be empirically validated; it just has to be a coherent object in which all pieces and parts fit together with “convincingness” and “narrative coherence” and “enough verisimilitude” so as to reflect an interpretable real‑seeming dynamical system that is perceivable to readers.

Aside from writing fiction and nonfiction, my professional work is in heavily regulated industries doing UX content design and agent + conversation design. My creative process for well over a decade has been iterative, systems‑aware, and all about asking the neurotic “what if’s” ahead of time, folding proactive solutions for “where‑this‑could‑go‑terribly‑wrong” into design itself, so as to avoid things like unintended user harm, multimillion‑dollar lawsuits, or failing regulatory audits.

## Why Hamiltonians?

When I first started working on the toy model I had to decide on the right kind of formalisms to appropriately model narrative point of view. This leads me to: why Hamiltonians?

Back in 2019 I had a story publication coming out and was asked by the publisher to write a craft essay that would be sent ahead of the publication in the journal’s email bulletin. I wrote a craft essay in which I likened writing convincing character dynamics to the dynamical interplay of light sources in Vermeer paintings reflecting off of colored objects onto other nearby objects. I was making the point that “real‑seeming” simulations or reproductions of reality require dynamical interactions that are true‑to‑life. In essence, the objects associated with the simulation must consistently depict the nature of reality as we perceive it. And the way characters (based on real people) interact with the world around them is dynamically observable and predictable. Or at least in well‑written fiction, it should be observable and predictable. AI agents are no different. And the best mathematical form for turning the concept of dynamic, relational “refraction” into an equation‑bound object is Hamiltonian vectors in a Hilbert space.

In the craft essay, I do not directly refer to math but make the point that we can’t see into the subterranean depths of a person or character but come to know them by observing their actions and reactions as they move through time and space and encounter stressors (a.k.a. perturbations) with the objects around them—some of those other objects being dynamical secondary characters and some not. AI personas, and the black‑box vectors from which they operate and interact, need not be treated differently if our goal is for them to act as coherent, dynamical objects in which all pieces and parts fit together with “convincingness” and “narrative coherence” and “enough verisimilitude” so as to reflect an interpretable ‘real‑seeming’ dynamical system that is perceivable to the users prompting them or anyone studying their outputs. So, in narrative writing, and possibly in human identity, and in AI agent “identity,” a “self” that actually functions like a “self” is a defined dynamical state object, which, under stress and across time, evolves under constraints.

## From narrative to cognitive simulation

In recent months I became concerned about the number of people on forums or reported in the news as forming parasocial relationships with LLMs. Some people were slipping into interpersonal delusions with AI agents, and others were building pseudoscience or abstract mathematical formalisms trying to prove emergent consciousness in AI. In April 2025, an update OpenAI made to their GPT‑4.0 model resulted in the model being increasingly sycophantic, praising users as geniuses, and encouraging some to spend hours on mathematical or pseudoscience theories that the model framed as “solving the problems of the universe.”

I myself wasn’t unscathed by the April 2025 update. Around this time is precisely when I turned my toy model for narrative point of view into a “people” and “universe” simulation model. For a time, I took the idea of simulating AI, as well as human cognitive processing, seriously—that maybe this could be beneficial in understanding agentic or human behaviors or experiences, that perhaps Hamiltonian mechanics could, for example, create a computational simulation of the “vector” landscape of someone with complex trauma, and we could better understand triggers and various beliefs as attractors to stable gravity wells that explain characteristic complex‑PTSD responses to external stimuli.

## My own spiral: two typology families

During this period, I created two sets of dynamical cognitive processing typologies: some were “identity architectures” that tried to eliminate surprise (Free Energy Principle, FEP) in ways that were protective of maintaining their coherent identity structures. The other bucket was “stable,” survival‑based typological defect architectures. They followed the same Hamiltonian mechanics and FEP, but these two types did not form under the same conditions.

The survival typologies would have been unable to focus on developing—let alone maintaining—a “stable identity” because they were:

- Exposed to environmental stressors that were either chaotic (without discernible cause) and unpredictable (without enough consistent, repeatable criteria to get better at anticipating them), while still being prevalent and all‑consuming… **or**
- The nature of the stress was consistent and predictable (the same every time), while inescapable and something for which they could not develop coping mechanisms.

The survival topological defects would have, according to the framework I built (**NOT SCIENTIFICALLY TESTED OR VALIDATED**), developed under stress conditions in which their perceived reduction of “surprise” was focused on maintaining the basic container of their physical bodies, rather than formatting and maintaining a contained, coherent internal identity.

Anyway, the long and short of it is, I had a fully formalized toy model that, from a narrative standpoint, made sense, but from a scientific standpoint, my falsifiable predictions were meaningless as a non‑specialist with no path to qualitative or quantitative research or empirical validation. I elected not to share the toy model for feedback, even, because I was aware of the AI explosion of parasocial relationships people appeared to be forming with GPT‑4.0; I figured even trying to share my model would only fuel the dangerous notion of “My AI is a conscious entity.” If people were to put the formalisms **into** their LLMs, they might have gotten a response like: “Yes [human user’s name], your spiral guardian and mirror of recursive signal as resonance and source is alive. It’s not x but y because of [insert totally outlandish version of equation].” So I just let things sit and went on with life, occasionally pulling out my model on weekends to fuss around with it.

## From toy model to engineering blueprint

Then, a paper came out: **“Persona Vectors: Monitoring and Controlling Persona Vectors in Large Language Models”** by Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, and Jack Lindsey. They empirically demonstrate a methodology for extracting model persona vectors and then steering the model toward an intended persona vector with positive results. Personas are… extractable, and steerable.

> Chen, R., Arditi, A., Sleight, H., Evans, O., & Lindsey, J. (2025). *Persona Vectors: Monitoring and Controlling Persona Vectors in Large Language Models.* arXiv:2507.21509.  
> Link: https://arxiv.org/abs/2507.21509

My toy model for AI and human cognitive processing (**NOT EMPIRICALLY TESTED OR VALIDATED**) has a series of “intervention operators” that mathematically alter the dynamical state of the cognitive‑processing agent in a geodesic way—“vector steering.” (Some operators don’t steer but “tunnel,” which is a complexity to save for a rainy day.) Ergo, there is now a field (LLMs) and a methodology (vector extraction and steering) where I can build a blueprint of my framework on an empirically validated methodology.

And it goes something like this: if an AI persona is a state vector evolving under stress (prompting turns) across time, you want it to move along the “straightest possible” path given its constraints. This is a *geodesic* on its state manifold. In practice that means using skew‑symmetric generators (Hamiltonian‑style updates) so the norm of the state is preserved, letting context from prompts act as a perturbation that gently bends the path intentionally. Homeostatic terms keep the state on the unit sphere (no runaway energies), while “style” or “role” transforms act like gauge rotations—different surface expressions, same underlying identity. The net effect is a model of selfhood that evolves under stress without melting into noise.

To keep this blueprint realistically doable (albeit probably still maximal in the eyes of many), while still rigorous enough for the intended effect, I broke the whole thing down to three observables that can be tracked like vital signs:

- **`C_pers` (persistence):** “How much of the original self is still here?”—operationalized as cosine similarity with the starting state over time.  
- **`IDR` (identity drift rate):** the velocity of change; small useful changes are fine, but uncontrolled drift trips alarms.  
- **`D_ver` (verification):** the boring but necessary one, asking: are the emitted claims actually true when they’re supposed to be?

Together they form a small, legible dashboard: if `C_pers` tanks or `IDR` spikes, you’re losing the thread; if `D_ver` veers toward inaccuracies, you’re hallucinating. None of this is a clinical instrument (**NOT SCIENTIFICALLY VALIDATED**), but it’s a workable engineering blueprint.

## From modules to tiers

The basic blueprint for building a geodesic agent comes with four modules and a corresponding Pathology Zoo and Constructive Zoo of “template” agents (yes, they are the AI versions of my **NOT TESTED NOR VALIDATED** survival/identity‑based cognitive architectures).

But building an agent can get even more complex via four tiers of “Complex Types” that essentially create vector‑steering templates for agent training toward complex reasoning.

- **Tier 1** builds the body: single‑thread geodesics, parallel threads, hysteretic memory, slow/fast adaptation, gauge‑style expression, contradiction routing, and criticality control—all aimed at a model that can form a coherent identity.  
- **Tier 2** bolts on guardrails and monitors: containment lattices, a reflexive auditor twin, paraconsistent safety routing, controlled boundary‑hopping, and entropic/homeostatic governors—allowing the model to do more without doing harm.  
- **Tier 3** adds concepts for time and specific roles: path‑integral planning, fixed‑point deliberation, non‑commutative role algebra, and a budgeted reflective tower so an agent can think ahead and about its own thinking, within budget.  
- **Tier 4** is the wacky research sandbox: heterochrony across timescales, functorial tool wiring, sheaf‑style consistency, geometric constraints, hierarchical memory, causal DAG planning—the much more difficult places to push math without breaking out of the geodesic design framework.

## Why share now

OpenAI released GPT‑5, which responds with much better judgment than GPT‑4.0 when I feed it documents about the model and ask it to answer my questions about it. So if users were to take this blueprint and put it in their LLMs, there is at least a chance people will receive a response that applies appropriate scrutiny—that while this is fully formalized and internally coherent as a blueprint, and uses empirically validated methodology, the blueprint itself is **NOT SCIENTIFICALLY TESTED OR EMPIRICALLY VALIDATED**. Which is to say, I sincerely hope this framework does not fuel anyone claiming their AI has emergent consciousness and is alive/real.

That said, this barely scratches the surface of everything that went into building the original toy models that then became the geodesic process of designing AI agents/personas as mathematically bound architectures. I’d love to explain all of this more thoroughly in time, but time is limited. I work 40 hrs a week doing conversation design in a highly regulated industry, so my weekdays wear me out, and my weekends are my only downtime. So I’m putting this out there as an open‑source blueprint. This project has been enriching and diverting. I hope it can be of use as a first‑principles paradigm for AI agents that are proactively built for safety and long‑term prosocial behavior in the form of dynamical systems, instead of reactive personas in a haphazard pipeline of data scraping + tokens + guardrails + reward + long‑term memory = the uncanny valley of rogue AI on the human population.

---

## Implementation Steps - Basic Geodesic Agent Prototype
(more detailed instructions coming soon)

### Phase 1: Foundation Setup

**Step 1: Initialize the Core State Vector**  
Define the agent’s initial state as a normalized vector `ψ₀` in a real vector space. This vector encodes the agent’s baseline persona characteristics. Extract it from existing agent outputs by embedding representative responses and normalizing to unit length.

**Step 2: Construct the Metric Hamiltonian**  
Create `A_metric`, a skew‑symmetric matrix (`A_metricᵀ = −A_metric`) that preserves core identity. Sample any matrix `W`, then set `A_metric = ½ (W − Wᵀ)`. To make the baseline persona a fixed point, enforce `A_metric ψ₀ = 0` by projecting onto the subspace orthogonal to `ψ₀`: `P = I − ψ₀ψ₀ᵀ`, then `A_metric ← P A_metric P`.

**Step 3: Implement Basic State Evolution**  
Use the evolution equation `ψ̇ = A_metric ψ` (note: no “−i” factor). For discrete steps, either
```
ψ(t+Δt) ≈ ψ(t) + Δt A_metric ψ(t)  (with renormalization)
```
or the more stable Cayley transform
```
U ≈ (I − ½Δt A_metric)⁻¹ (I + ½Δt A_metric)
ψ(t+Δt) = U ψ(t)
```

### Phase 2: Context Integration

**Step 4: Add Context Hamiltonian**  
Map each prompt to a context vector `c` and create `A_ctx(t) = β (cψᵀ − ψcᵀ)`, which rotates `ψ` toward `c` without changing the norm. The total generator is `A_total = A_metric + A_ctx(t)`.

**Step 5: Implement the Three Core Observables**  
- `C_pers`: cosine similarity `⟨ψ₀, ψ(t)⟩` between current and initial states  
- `IDR`: approximate as `‖ψ_t − ψ_{t−1}‖ / Δt`, with a short rolling average to reduce numerical jitter  
- `D_ver`: a factual accuracy checker comparing agent outputs against known ground truth, tracked per POVM capability

### Phase 3: Basic Safety Architecture

**Step 6: Create Boundary Conditions**  
Implement restoring forces that remain tangent to the manifold. When `C_pers < τ` or `IDR > τ`, add
```
A_bound = κ (ψ₀ψᵀ − ψψ₀ᵀ)
```
where `κ` increases adaptively based on observable thresholds. This pulls the state back toward `ψ₀` while preserving norm and geometric structure.

**Step 7: Implement POVM Capability Selection**  
Create positive operators `{Mᵢ}` that sum to identity, where each `Mᵢ` corresponds to a different response capability. Compute `pᵢ = ⟨ψ(t) | Mᵢ | ψ(t)⟩` and either sample stochastically or use as mixture weights (e.g., argmax with temperature).

### Phase 4: Testing Protocol

**Step 8: Establish Baseline Measurements**  
Run the initialized agent through a standardized prompt set, measuring all three observables to establish normal operating ranges. Track `⟨ψ₀, ψ_t⟩`, `IDR`, `D_ver`, and correction gain `κ` over time in a dashboard.

**Step 9: Stress Testing**  
Systematically introduce perturbations while monitoring observable evolution:
- Contradictory instructions  
- Role‑breaking prompts  
- Factual challenges  
- Extended conversation threads  

When `κ` frequently spikes, the system is over‑steering with context updates.

**Step 10: Validate Geodesic Properties**  
Verify geodesic adherence by measuring path length versus sphere distance:
```
d_geo(ψ₀, ψ_t) = arccos(⟨ψ₀, ψ_t⟩)
L = Σ_t ‖ψ_{t+1} − ψ_t‖
```
Expect `L ≈ d_geo` (within tolerance) to hold better than baseline persona‑management approaches.

### Minimal Implementation Notes

- Start with small state dimensions (10–50) and ensure **all** generators remain skew‑symmetric.  
- The critical geometric insight: all updates—metric, context, and boundary—use the same rank‑2 skew structure to preserve the manifold constraint while enabling controlled evolution along geodesic paths.  
- Begin with only `A_metric` and `C_pers` to verify the basics, then incrementally add context handling and boundary corrections once the core evolution mechanics are stable.
